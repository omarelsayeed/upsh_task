{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":992592,"sourceType":"datasetVersion","datasetId":543826},{"sourceId":2691304,"sourceType":"datasetVersion","datasetId":1638607},{"sourceId":4225259,"sourceType":"datasetVersion","datasetId":2490412},{"sourceId":7145562,"sourceType":"datasetVersion","datasetId":3734669},{"sourceId":8533833,"sourceType":"datasetVersion","datasetId":4968045},{"sourceId":8937196,"sourceType":"datasetVersion","datasetId":5371246},{"sourceId":134252919,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade datasets\n!pip install --upgrade evaluate\n!pip install accelerate -U\n!pip install --upgrade transformers[torch]","metadata":{"id":"SJdExTXp1EGx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel , AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass DatasetPrep(torch.utils.data.Dataset):\n   def __init__(self, texts, labels):\n       self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=50)\n       self.labels = labels\n   def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        label = self.labels[idx] if self.labels is not None and idx < len(self.labels) else None\n        if label is not None:\n            item[\"label\"] = torch.tensor(label)\n        return item\n   def __len__(self):\n       return len(self.encodings[\"input_ids\"])\n    \nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nimport numpy as np\n\ndef compute_metrics(p):\n   pred, labels = p\n   pred = np.argmax(pred, axis=1)\n   accuracy = accuracy_score(y_true=labels, y_pred=pred)\n   recall = recall_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n   precision = precision_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n   f1 = f1_score(y_true=labels, y_pred=pred, pos_label='positive', average='weighted')\n   return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_external_datasets():\n    revs = pd.read_csv(\"/kaggle/input/arabic-100k-reviews/ar_reviews_100k.tsv\" , delimiter = \"\\t\")\n    company_rates = pd.read_csv(\"/kaggle/input/d/fahdseddik/arabic-company-reviews/CompanyReviews.csv\")\n    company_rates = company_rates[[\"review_description\" , \"rating\"]]\n    company_rates.columns = [\"text\" , \"label\"]\n    company_rates = company_rates[company_rates.label.isin([1,-1])]\n    mapping = {1 : \"Positive\" , -1 : \"Negative\"}\n    company_rates.label = company_rates.label.map(mapping)\n\n    merged_df = pd.concat((revs , company_rates))\n\n    twitter = pd.read_excel(\"/kaggle/input/arabictwittercorpusajgtmaster/AJGT.xlsx\")\n    twitter.drop(\"ID\" , axis = 1 ,  inplace=True)\n    twitter.columns = [\"text\" , \"label\"]\n    twitter.label.value_counts()\n\n    merged_df = pd.concat((merged_df , twitter))\n    from datasets import load_dataset\n\n    tw_test = pd.DataFrame(load_dataset(\"asas-ai/Arabic_Sentiment_Twitter_Corpus\")[\"test\"])\n    tw_train = pd.DataFrame(load_dataset(\"asas-ai/Arabic_Sentiment_Twitter_Corpus\")[\"train\"])\n    twitter_dataset = pd.concat((tw_test , tw_train))\n    twitter_dataset.columns = [\"label\" , \"text\"]\n    mapping = {\"neg\" : \"Negative\" , \"pos\" : \"Positive\"}\n    twitter_dataset.label = twitter_dataset.label.map(mapping)\n    twitter_dataset\n\n    merged_df = pd.concat((merged_df , twitter_dataset))\n    return merged_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset():\n    dataset = pd.read_csv(\"/kaggle/input/semngr/sentiment task - Sheet1.csv\")\n    dataset = dataset.dropna()\n    dataset.columns = ['text' , 'label']\n    dataset = dataset[~dataset.text.str.contains(\"ارين\")]\n    dataset['text'] = dataset.text.apply(lambda x : post_process_summary(x))\n    from sklearn.model_selection import train_test_split\n    dataset , eval_dataset = train_test_split(dataset , test_size = 0.2 , stratify=dataset['label'] , random_state = 42)\n    dataset , left_out_test_set = train_test_split(dataset , test_size = 0.2 , stratify=dataset['label'] , random_state = 42)\n    unique_labels = dataset['label'].unique()\n\n    # Create a contiguous mapping dictionary automatically\n    contiguous_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n    dataset.label =  dataset['label'].map(contiguous_mapping)\n    reversedDict = {val:key for key , val in contiguous_mapping.items()}\n    # eval_dataset = pd.read_csv(\"/kaggle/input/classifier-data/5K Balanced Evaluation Benchmark.csv\")[[\"text\", \"label\"]]\n    # eval_dataset.columns = [\"text\" , \"label\"]\n    eval_dataset.label =  eval_dataset['label'].map(contiguous_mapping)\n    dataset = dataset.sample(frac = 1)\n    eval_dataset = eval_dataset.sample(frac = 1)\n    \n    dataset2 = generate_external_datasets()\n    dataset2 , left_out_test_set_for_online = train_test_split(dataset2 , test_size = 0.2 , stratify=dataset2['label'] , random_state = 42)\n\n    dataset2.label = dataset2.label.map(contiguous_mapping)\n    dataset = pd.concat((dataset , dataset2))\n    dataset = dataset.sample(frac=1)\n    dataset = dataset.dropna()\n    dataset = dataset.drop_duplicates(subset = [\"text\"])\n    \n    from datasets import Dataset\n    train_set = Dataset.from_pandas(dataset)\n    test_set = Dataset.from_pandas(eval_dataset)\n\n    train_dataset = DatasetPrep(train_set[\"text\"], train_set['label'])\n    test_dataset = DatasetPrep(test_set[\"text\"], test_set['label'])\n    \n    return train_dataset , test_dataset , left_out_test_set , reversedDict, left_out_test_set_for_online","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key = \"\")\nfrom huggingface_hub import login\nlogin(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [  \n#             \"Omartificial-Intelligence-Space/Arabert-all-nli-triplet-Matryoshka\" ,   \n#             \"Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet\" ,\n#             \"CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment\",\n#             \"CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment\" , \n#             \"CAMeL-Lab/bert-base-arabic-camelbert-ca-sentiment\" , \n#             \"nourmorsy/PermoBERT-Arabic-Sentiment-Analysis-NoFarasa-WLV-44000Token\" , \n#             \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\" , \n#             \"ssary/XLM-RoBERTa-German-sentiment\" , \n#             \"Ammar-alhaj-ali/arabic-MARBERT-sentiment\" , \n#             \"sentence-transformers/distiluse-base-multilingual-cased-v2\" , \n#             \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" ,\n#             \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n#             \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n#             \"aubmindlab/bert-base-arabertv02\",\n#             \"UBC-NLP/MARBERTv2\"   ,\n#             \"intfloat/multilingual-e5-large\" , \n#             \"intfloat/multilingual-e5-small\"\n              \"omarelsayeed/setfit_ammar\"\n]  \n\n\nexperiment_tracker = []\nimport torch\nimport gc\n\nfor MODEL_NAME in models:\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        model = (AutoModelForSequenceClassification.from_pretrained(MODEL_NAME , num_labels = 4 , ignore_mismatched_sizes= True).to(\"cuda\"))\n\n        train_dataset , test_dataset, left_out_test_set , reversedDict , left_out_test_set_for_online = prepare_dataset()\n\n        from transformers import Trainer, TrainingArguments\n\n        batch_size = 250\n\n        args = TrainingArguments(\n          output_dir=\"my_awesome_model\",\n          eval_strategy=\"epoch\",\n          per_device_train_batch_size=batch_size,\n          per_device_eval_batch_size=128,\n          num_train_epochs=2,\n          learning_rate=5e-5,\n          logging_steps =12,\n          warmup_ratio = 0.2\n        )\n\n        from transformers import Trainer\n\n        trainer = Trainer(\n          model = model ,\n          args = args ,\n          train_dataset = train_dataset ,\n          eval_dataset = test_dataset ,\n          compute_metrics=compute_metrics\n        )\n\n        results = trainer.train()\n        eval_results = trainer.evaluate()\n\n        model.config.id2label = reversedDict\n        model.save_pretrained(f\"exp_{MODEL_NAME}\")\n        tokenizer.save_pretrained(f\"exp_{MODEL_NAME}\")\n\n        pipe = pipeline(\"text-classification\" ,f\"exp_{MODEL_NAME}\" , device= \"cuda\")\n        predictions = pipe.predict(left_out_test_set.text.tolist())\n        pred , score = [p['label'] for p in predictions] , [p['score'] for p in predictions]\n        left_out_test_set['predictions'] = pred\n        left_out_test_set['score'] = score\n\n        test_accuracy = left_out_test_set[left_out_test_set.label == left_out_test_set.predictions].shape[0]/left_out_test_set.shape[0]\n        print(\"Test set accuracy = \" , test_accuracy)\n\n        experiment_tracker.append(\n            {\n                \"test_accuracy\" : test_accuracy, \n                \"model_name\" : MODEL_NAME , \n                \"training_loss\" : results.training_loss ,\n                \"eval_loss\" : eval_results[\"eval_loss\"] , \n                \"eval_f1\" : eval_results['eval_f1']\n            }\n        )\n\n        del model , tokenizer , pipe\n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"id":"yVE-aod-1EG5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"\")\ntokz = AutoTokenizer.from_pretrained()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"\")\ntokz.push_to_hub(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}